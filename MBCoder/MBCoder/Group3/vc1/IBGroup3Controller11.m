//
//  IBGroup3Controller11.m
//  MBCoder
//
//  Created by 叶修 on 2025/2/6.
//  Copyright © 2025 inke. All rights reserved.
//

#import "IBGroup3Controller11.h"

@interface IBGroup3Controller11 ()

@end

@implementation IBGroup3Controller11

- (void)viewDidLoad {
    [super viewDidLoad];
    // Do any additional setup after loading the view.
}

@end

/*
 码率( 比特率):指的是单位时间内传输或处理的比特数，单位通常是kbps(千比特每秒)或者Mbps(兆比特每秒)

 一、音频
 1、声音量化基本概念
 采样大小：一个采样点用多少bit存放，常用 16bit
         采样大小决定了每个采样点能够表示的精度和动态范围，直接影响音频的动态范围，即声音中最大音量和最小音量之间的差距
 
 采样率：8k，16k，32k，44.1k，48k，
      即每秒钟从连续的模拟信号中提取多少个离散的样本点来进行数字化处理
 
 声道数：单声道，双声道，多声道(立体声)
       多声道: 5.1 声道（左、中、右、左环绕、右环绕和低频效果声道)
              7.1 声道（在 5.1 声道基础上增加了后中置声道和侧环绕声道）
              主要用于家庭影院、电影院等需要更沉浸式音频体验的场景，能够提供更丰富的音频空间信息。

 比特率：采样大小*采样率*声道数
 
 2、频域遮蔽效应，时域遮蔽效应
 
 3、熵编码：哈夫曼编码，算术编码，香农编码
 
 4、创建的音频编码器
 - 原始数据：PCM
 - 多声道：AC3，DTS
 - 无损编码：FLAC，ALAC，WAV，APE
 - 有损编码：MP3，AAC，Opus
   Opus:低延迟（<50ms）、动态码率（6kbps~510kbps），支持语音与音乐混合优化，应用 WebRTC，直播
   AAC: MP3的继任者，效率更高（相同码率下音质优于MP3），应用点播，YouTube、Netflix
 - 如何选择：
   存档/专业用途：FLAC或WAV（兼容性优先）。
   日常听歌：AAC（256kbps以上）或 Opus（透明音质约160kbps）。
   实时通信：Opus（自适应码率+低延迟）。
   多声道影视：AC3（兼容性）或DTS-HD（追求音质）。
 
 5、AAC
 1）编码规格
 - LC-AAC（Low Complexity AAC）：这是最基本和常用的 AAC 规格，复杂度较低，在中低比特率下能提供较好的音质，适用于大多数日常音频应用，如在线音乐、手机铃声等
 - HE-AAC（High Efficiency AAC）：在 LC-AAC 基础上通过采用 SBR（Spectral Band Replication）技术扩展了编码带宽，提高了编码效率，在低比特率下能获得更好的音质
 - HE-AAC V2：在 HE-AAC 的基础上进一步引入了 PS（Parametric Stereo）技术，通过传输立体声参数而不是完整的立体声信号来节省带宽，进一步提高了编码效率
 
 2）AAC 常见的采样率：
 - 44.1kHz：这是音频 CD 的标准采样率，也是音乐制作和消费中最常用的采样率之一，能够很好地还原人耳可听范围内的音频信号，适用于大多数音乐和音频内容。
 - 48kHz：常用于专业音频录制和视频制作领域，与视频的帧率配合较好，在一些广播和影视制作中较为常见。
 - 32kHz：适用于一些对带宽要求较高或对音频质量要求不是特别苛刻的场景，如某些移动音频应用或网络广播等。
 - 22.05kHz 和 11.025kHz：通常用于语音通信或对音频文件大小要求非常严格的情况，如一些低比特率的语音留言或早期的移动设备音频应用。
 
 3) AAC 常见的比特率
 - 64kbps：在这个比特率下，AAC 可以提供基本的音频质量，适用于一些对音频质量要求不高、带宽有限的场景，如某些在线语音聊天或低质量音频流。
 - 128kbps 和 192kbps：这是比较常见的比特率，能够提供较好的音质，在一般的音乐播放和网络音频传输中较为常用，能在文件大小和音质之间取得较好的平衡。
 - 256kbps 及以上：对于对音质要求较高的用户和应用，如专业音乐制作、高保真音频流等，可以选择更高的比特率，以获得更接近原始音频的质量。
 
 4）AAC音频格式
 - ADIF格式：音频数据交换格式：只有一个统一的头，必须得到所有数据后解码，适用于本地文件。
 - ADTS格式：特点是存在同步字节(12bits，0xFFF)的标识，而且每一帧AAC都有头信息，因此可以从任意帧和位置开始解码，比较适合直播流场景中
 
 5）文件扩展名：.aac、.m4a、.mp4（容器格式）
 
 6、音频重采样
 - 原因
   从设备采集的音频数据与编码器要求的数据不一致
   扬声器要求的音频数据与要播放的音频数据不一致
 
 二、视频
 
 1、视频量化基本概念
 帧率：每秒显示的帧数。比如电影通常是24fps

 2、分辨率
 分辨率名称        分辨率（宽×高）    宽高比    典型应用场景
 360p              640×360        16:9    低清视频、老旧设备
 480p              640x480        4:3     老式电视、VCD、早期数码相机
 480p              854×480        16:9    低带宽直播、移动端视频
 540p              960×540        16:9    移动端直播、低清视频流
 720p (HD)         1280×720       16:9    主流直播、在线视频
 1080p (FHD)       1920×1080      16:9    高清直播、电影、赛事
 1440p (2K/QHD)    2560×1440      16:9    高端游戏、专业直播
 2160p (4K/UHD)    3840×2160      16:9    超高清视频、高端直播
 4320p (8K)        7680×4320      16:9    实验性超高清内容
 
 3、帧率
 - 24 FPS —— 电影标准帧率，呈现电影质感，常用于影视剧和影院级内容。
 - 25 FPS —— 欧洲电视标准（PAL制式），用于广播电视、纪录片等。
 - 30 FPS —— 常用于网络视频、高清电视（NTSC制式），如 YouTube、直播、家庭录像。
 - 48 FPS —— 电影增强帧率，如《霍比特人》采用48 FPS，提高流畅度。
 - 50 FPS —— 欧洲高帧率电视标准，用于体育赛事或高动作内容。
 - 60 FPS —— 常见于高帧率游戏视频、运动视频、4K流媒体，增强流畅度。
 - 120 FPS —— 超高帧率，用于慢动作拍摄、电竞直播、VR视频。
 - 240 FPS 或更高 —— 专业慢动作拍摄，主要用于高端设备（如 iPhone、GoPro 等慢动作模式）。
 
 4、码流
 1）理论码流计算，有助于在视频编码设计或实时传输时，计算必要的带宽需求
 码流 (bps) = 分辨率 × 帧率 (FPS) × 色深 (bit/pixel) × 压缩比
 各参数说明：
 分辨率：图像的宽 × 高（如1920×1080）。
 帧率：每秒的帧数（如30FPS、60FPS）。
 色深：每个像素的比特数，常见为12位（YUV 4:2:0）。
 压缩比：与编码格式相关，H.264 和 H.265 压缩比差异较大
 
 2）实际码流计算，用于已知视频文件的分析，分析视频文件的存储效率和传输需求
 码流(bps)= 视频时长 (s) / 文件大小 (bit)
 
 
 5、YUV（YCbCr）
 是一种颜色编码方式，常用于视频压缩、传输和处理。YUV将图像数据分为亮度（Y）和色度（U和V）分量，能够有效减少数据量，同时更好地兼容人类视觉感知。
 
 1）YUV的基本组成
 Y（Luminance）亮度分量：描述图像的明暗信息。
 U（Chrominance）色度分量1：描述蓝色与亮度的差值（Blue - Y）。
 V（Chrominance）色度分量2：描述红色与亮度的差值（Red - Y）。
 相比RGB，YUV更加节省存储空间，并且适合视频压缩编码，因为人眼对亮度更敏感，对色度变化不太敏感
 
 2）常见YUV格式
 采样模式       亮度 (Y)        色度 (U、V)        特点                   用途
 YUV 4:4:4    每像素采样一次    每像素采样一次       无压缩，高质量          专业影视后期处理
 YUV 4:2:2    每像素采样一次    每2个像素采样一次    色度压缩较小            高清电视、广播视频
 YUV 4:2:0    每像素采样一次    每4个像素采样一次    色度压缩显著，节省空间    网络视频、蓝光光盘、监控
 
 3）YUV和RGB关系
 - RGB 用于屏幕的展示
 - YUV 用于采集和编码
 
 4）YUV存储格式
 图文详解YUV420 数据格式：
 https://markrepo.github.io/avcodec/2018/06/28/YUV/
 
 6、H264视频编码
 
 1）码流：
 https://doc.shengwang.cn/doc/rtc/ios/basic-features/video-profile
 
 2）GOP(Group of Pictures):
    GOP是一组连续的帧，包含关键帧（I帧）、预测帧（P帧） 和 双向预测帧（B帧）
 
 3)GOP的结构
   GOP的结构由帧类型组成：
   - I 帧 (Intra Frame)
     · 关键帧，包含完整的图像信息，可独立解码。采用帧内压缩技术
     · 大小最大，但数量最少。
     · 作用：作为参考帧，提供恢复画面的基准。
   - P 帧 (Predicted Frame)
     · 预测帧，基于之前的I帧或P帧进行预测编码。采用帧间压缩技术
     · 大小中等，包含差异数据（变化信息）。
   - B 帧 (Bidirectional Frame)
     · 双向预测帧，参考前后的I帧或P帧，压缩效率最高。
     · 大小最小，但解码复杂。
   
 4）IDR 帧（Instantaneous Decoding Refresh Frame）
    是视频编码中一种特殊的 I帧（关键帧），它的作用是重置解码器的参考帧缓冲区，从而确保后续帧不依赖于之前的帧
 
 IDR帧的主要作用
 - 强制解码器刷新
   · 解码器在遇到IDR帧时，丢弃之前所有帧的参考数据，并从该IDR帧开始重新解码后续的P帧和B帧。
   · 这样可以防止错误传播，确保解码后的图像正确。
 
 - 视频的随机访问点（Seek Point）
   · IDR帧是视频的关键随机访问点，允许播放器在此处开始解码而不会出现画面错误。
   · 适用于跳转播放、快进、直播开始点等场景。
 
 - 提高解码和重同步能力
   · 在网络传输中（如流媒体、直播），IDR帧有助于在丢包或解码错误后快速恢复画面。
   · 实时应用中，IDR帧有助于减少延迟和卡顿。
 
 - 减少错误传播
   · 如果视频传输中出现丢帧或解码错误，IDR帧能有效阻止错误扩散到后续帧，保证视频质量。
 
 5) H264压缩技术
 - 空域压缩（空间冗余去除）：利用图像中像素之间的相似性，减少空间冗余，主要方法：帧内预测
 - 时域压缩（时间冗余去除）：减少连续帧之间的冗余信息，利用前后帧的相似性进行编码，主要方法：帧间预测
 - 变换编码与量化：
   · 使用4×4整数变换（类似于离散余弦变换，DCT），将图像从时域转换到频域，集中能量分布。
   · 通过量化（Quantization）将小的系数归零，减少存储数据。量化强度决定了压缩比和画质的平衡。
 - 熵编码（数据冗余去除）：熵编码是H.264中的无损压缩阶段，进一步减少数据量，主要方法：CABAC
 
 6）宏块
 宏块（Macroblock，MB）是视频帧的基本处理单元，用于进行帧内预测、帧间预测、变换、量化和熵编码等操作。
 
 H.264支持将一个宏块细分为更小的块，提高编码灵活性和压缩效率。分割模式包括：
 16×16（完整宏块）
 16×8、8×16（适合运动区域）
 8×8、4×4（适合复杂细节区域）
 
 7）H264码流分层
 - NAL 层：视频数据网络抽象层，处理乱序，丢包等情况
 - VCL 层：视频数据编码层，结构关系：图片（frame） ->  片（slice） -> 宏块（MB）-> 子块
 +------------------+------------------+
 | NAL Header (1字节) | NAL Payload (RBSP)  |
 +------------------+------------------+

 8）码流基本概念
 - SODB（数据比特串）是编码后的纯视频压缩数据，即未经封装的比特流，包含实际的预测、变换、量化、熵编码后得到的比特数据。由VCL层产生
 - RBSP（原始字节序列载荷）是在SODB基础上进行字节对齐和填充的比特流，用来保证数据的完整性和可解码性
 - NALU（网络抽象层单元）是H.264码流中的基本传输单元，它将RBSP封装成一个结构化的单元，方便网络传输和存储

 9）码流格式
 特性         Annex B 格式                         RTP 格式
 起始码       使用 0x000001 或 0x00000001           无起始码，使用RTP头部
 NALU封装     独立NALU，直接存储或传输                NALU封装在RTP包中，支持分片
 冗余数据     起始码增加冗余                         无起始码，节省带宽
 解析复杂度   简单直观                               需处理RTP头部和分片逻辑
 适用场景     文件存储、广播系统                       实时流媒体、网络传输
 典型协议     MP4、TS                               RTP/RTCP、SRT、WebRTC
 
 10）SPS 的 Profile 和 Level
 
 Profile
 定义了编码和解码视频时使用的工具和特性，它规定了编码器可以使用的编码工具和技术，以及解码端需要支持的功能集合，
 通过限制编码工具的使用范围来实现不同的编码效率、图像质量和复杂度的平衡，以适应不同的应用场景和设备能力。
 H.264 中的常见 Profile：
 - Baseline Profile：
   主要用于实时通信和低延迟应用，如视频会议、视频监控等。它支持一些基本的编码工具，如 I 帧和 P 帧编码、部分运动估计和补偿等，
   不支持 B 帧和一些高级的编码特性，具有较低的编码复杂度和较好的容错性。
 - Main Profile：
   适用于大多数传统的视频应用，如标准清晰度电视（SDTV）和高清电视（HDTV）的广播、视频存储等。
   在 Baseline Profile 的基础上，增加了对 B 帧编码、加权预测等功能的支持，能够提供更好的图像质量和编码效率。
 - High Profile：
   主要用于高清和超高清视频应用，如蓝光光盘、高清视频流媒体等。它进一步增加了更多的编码工具，如多参考帧、自适应环路滤波等，
   能在高分辨率和高码率下实现更高的编码效率和更好的图像质量。
 
 Level 
 是对视频编码的具体参数和性能进行量化限制的规定，它定义了视频编码的最大分辨率、帧率、码率、缓冲器大小等参数的上限，
 用于确保编码后的视频流在不同设备和网络环境下能够正确解码和播放，不同的 Level 对应着不同的视频处理能力和资源需求
 H.264 中的 Level 举例
 Level 1：适用于低分辨率、低码率的视频应用，如手机视频等。通常限制最大分辨率为 176×144，最大帧率为 15fps，最大码率为 128kbps 等。
 Level 3.1：常用于标准清晰度视频，可支持最大分辨率为 720×576，帧率可达 25fps，码率上限相对较高，一般为 2Mbps 左右。
 Level 5.1：主要针对高清和超高清视频，可支持 4K 分辨率（3840×2160），帧率可高达 60fps，码率可达到几十 Mbps 甚至更高。
 
 Profile 和 Level 的关系
 Profile 和 Level 共同决定了视频编码的具体特性和适用范围。Profile 规定了编码的功能集合，Level 则在 Profile 的基础上对具体的参数进行限制
 
 11） PPS 的重要参数
 
 12） Slice 的 Header
 - 帧类型
 - GOP中解码帧序号
 - 预测权重
 - 滤波
 
 13）数据流分析工具（Elecard Stream Eye）
 x264 参数分类：预设值，帧参数，码流控制，编码分析，输出
 
 7、RTMP 协议 和 FLV 协议
 RTMP 是传输协议，负责实时传输音视频数据
 FLV 是封装格式，用于存储或传输音视频内容。
 两者常结合使用：RTMP 负责从推流端到服务器的数据传输，而 FLV 则是这些数据在传输或存储时的组织形式
 
 
 8、HLS 协议、 M3U8 和 TS 文件
 
 1）HLS 协议
 HLS是由苹果公司提出的一种基于HTTP的流媒体传输协议，它主要用于在互联网上传输视频和音频内容，特别适用于直播（Live）和点播（VOD）场景。
 HLS通过将音视频流切割成一系列小的TS切片，并生成M3U8播放列表文件，实现了流媒体的高效传输和播放。
 
 2）M3U8
 M3U8文件是HLS协议中的核心组成部分，它是一种基于文本的播放列表文件格式，用于描述媒体文件的位置和顺序
 
 3）TS
 TS文件是HLS协议中用于存储音视频数据的文件格式。它是一种容器格式，能够封装视频、音频和其他数据到一个文件中，方便存储和传输。
 TS文件由众多固定长度的数据包构成，这种结构设计让音视频能在文件中高效混合且同步播放。
 
 4）HLS 支持的功能：
 - 分片播放
 - 使用 HTTPS 加密 ts 文件
 - 快/倒放
 - 广告插入
 - 不同分辨率视频切换
 
 5）HLS 的弊端：10-30 秒（高延迟）
 - TCP 握手，
 - m3u8 文件下载，
 - m3u8 下的 ts 文件下载
 
 9、RTMP 和 HLS 对比
 特性          RTMP                    HLS
 延迟          1-5 秒（低延迟）          10-30 秒（高延迟）
 协议基础       TCP（专用流媒体协议）      HTTP（通用协议）
 兼容性        依赖播放器支持（如 Flash）  全平台兼容（浏览器、移动端）
 适用场景      实时直播、互动场景          点播、高网络适应性场景
 自适应码率    不支持                    支持
 服务器要求    需专用 RTMP 服务器         普通 HTTP 服务器即可
 
 选择建议
 RTMP：适合低延迟、高实时的场景（如赛事直播、在线教学）。
 HLS：适合高兼容性、自适应码率的场景（如点播平台、移动端观看）。
 现代直播系统常结合两者：用 RTMP 采集推流，再转封装为 HLS 分发，兼顾实时性和兼容性。
 
 10、WebRTC 和 RTMP

 1）对比
 特性           WebRTC                   RTMP
 延迟           毫秒级（<500ms）           秒级（1-5秒）
 传输层协议      UDP（QUIC）+ RTP/RTCP     TCP
 网络适应性      动态调整码率、支持P2P直连    依赖服务器中转
 数据封装        无封装或轻量 RTP 封装       FLV 格式封装
 主要场景        视频会议、实时通信          单向直播、点播
 
 2）WebRTC 合流
 
 2.1) 服务端合流原因：
 - 带宽和网络压力：对于观众端，如果需要同时播放多个流，网络压力会更大
 - 客户端性能压力：每个客户端需要同时解码和渲染多个音视频流，对 CPU、GPU 和内存资源消耗较大
 - 同步问题：不同参与者的音视频流可能由于网络延迟或时钟不同步，导致音画不同步
 - 布局和显示问题：需要额外的逻辑实现画中画、网格布局或动态切换
 - 成本问题：不合流时，服务器需要同时转发多个流，增加了带宽和服务器资源消耗。
 - 音频混音问题：不合流时，客户端需要自行处理多个音频流的混音，需要实现音频降噪、回声消除和音量平衡。
 
 2.2) 服务端合流
 - SFU（Selective Forwarding Unit）服务器：延迟低至 200~500ms
   SFU服务器作为媒体流的中转站，可以接收来自不同参与者的音视频流，并将它们转发给其他参与者,
   在SFU架构中，合流通常是在服务端进行的，因为SFU服务器可以对音视频流进行高效的处理和转发，减轻了客户端推流到多个观众端的压力
   例如：Agora、声网、腾讯云等提供的合流服务
 - MCU（Multipoint Control Unit）服务器：延迟通常 3~10 秒
   MCU服务器不仅具有SFU的功能，还可以对音视频流进行混合、切换、录制等处理。在MCU架构中，合流也是在服务端进行的，但MCU提供了更丰富的音视频处理功能
   例如：Zoom、WebEx 等视频会议系统
 
  直播连麦合流设计是 SFU（主播-连麦者实时交互） + MCU/CDN（观众合成流分发） 的混合架构，核心目标在于：
  - 主播与连麦者：优先保障低延迟交互（SFU）。
  - 观众：优先保障高并发分发和流畅体验（MCU + CDN）
 
 2.3) 客户端合流方案

 11、推流步骤
 - 解析flv文件
 - 获取音视频数据
 - 利用 librtmp 进行推流
 
 12、音视频的直播架构
 - 泛娱乐化直播
 - 实时互动直播
 
 13、流媒体服务器（SRS，Nginx-RTMP）
 推流端：
 摄像头 → 编码 → FLV封装 → RTMP推流 → 服务器
 播放端：
 播放器 ← RTMP拉流（FLV数据）、HLS拉流 ← 服务器

 直播中视频编解码、推拉流等流程解析
 https://www.jianshu.com/p/83fa6679e4a4
 
 视频直播技术干货(一)：揭秘百万级粉丝互动的Facebook实时视频直播
 http://www.52im.net/thread-541-1-1.html

 WebRTC → 深入分析各行业直播方案与原理
 https://juejin.cn/post/7219124460804718649
 
 
*/

